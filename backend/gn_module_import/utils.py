import os
from io import StringIO
import csv
import json
from datetime import datetime

from flask import current_app
from sqlalchemy import func
from chardet.universaldetector import UniversalDetector
from sqlalchemy.sql.expression import select, update, insert, literal
from sqlalchemy.sql import column
from sqlalchemy.orm import aliased
import sqlalchemy as sa
import pandas as pd
import numpy as np
from sqlalchemy.dialects.postgresql import insert as pg_insert, array_agg, aggregate_order_by
from geoalchemy2.functions import ST_Transform, ST_GeomFromWKB, ST_Centroid
from werkzeug.exceptions import BadRequest
from geonature.utils.env import db

from gn_module_import import MODULE_CODE
from gn_module_import.models import (
    TImports,
    BibFields,
    ImportSyntheseData,
    ImportUserError,
    ImportUserErrorType,
)

from geonature.core.gn_commons.models import TModules
from geonature.core.gn_synthese.models import Synthese, corAreaSynthese
from ref_geo.models import LAreas, BibAreasTypes
from pypnnomenclature.models import TNomenclatures, BibNomenclaturesTypes
from apptax.taxonomie.models import Taxref
from pypn_habref_api.models import Habref


def get_file_size(f):
    current_position = f.tell()
    f.seek(0, os.SEEK_END)
    size = f.tell()
    f.seek(current_position)
    return size


def detect_encoding(f):
    position = f.tell()
    f.seek(0)
    detector = UniversalDetector()
    for row in f:
        detector.feed(row)
        if detector.done:
            break
    detector.close()
    f.seek(position)
    return detector.result["encoding"]


def detect_separator(f, encoding):
    position = f.tell()
    f.seek(0)
    sample = f.readline().decode(encoding)
    if sample == '\n': #files that do not start with column names
         raise BadRequest("File must start with columns")
    dialect = csv.Sniffer().sniff(sample)
    f.seek(position)
    return dialect.delimiter


def get_valid_bbox(imprt):
    stmt = db.session.query(
        func.ST_AsGeojson(func.ST_Extent(ImportSyntheseData.the_geom_4326))
    ).filter(
        ImportSyntheseData.imprt == imprt,
        ImportSyntheseData.valid == True,
    )
    (valid_bbox,) = db.session.execute(stmt).fetchone()
    if valid_bbox:
        return json.loads(valid_bbox)


def insert_import_data_in_database(imprt):
    columns = imprt.columns
    extra_columns = set(columns) - set(imprt.fieldmapping.values())
    csvfile = StringIO(imprt.source_file.decode(imprt.encoding))
    csvreader = csv.DictReader(
        csvfile, fieldnames=columns, delimiter=imprt.separator
    )
    header = next(csvreader, None)  # skip header
    for key, value in header.items():  # FIXME
        assert key == value
    fields = BibFields.query.filter_by(autogenerated=False).all()
    fieldmapping = {
        field.source_column: imprt.fieldmapping[field.name_field]
        for field in fields
        if (
            field.name_field in imprt.fieldmapping
            and imprt.fieldmapping[field.name_field] in columns
        )
    }
    obs = []
    line_no = 0
    for row in csvreader:
        line_no += 1
        assert list(row.keys()) == columns
        o = {
            "id_import": imprt.id_import,
            "line_no": line_no,
        }
        o.update(
            {
                dest_field: row[source_field]
                for dest_field, source_field in fieldmapping.items()
            }
        )
        o.update(
            {
                "extra_fields": {col: row[col] for col in extra_columns},
            }
        )
        obs.append(o)
        if len(obs) > 1000:
            db.session.bulk_insert_mappings(ImportSyntheseData, obs)
            obs = []
    if obs:
        db.session.bulk_insert_mappings(ImportSyntheseData, obs)
    return line_no


def do_nomenclatures_mapping(imprt, fields):
    # Set nomenclatures using content mapping
    for field in filter(lambda field: field.mnemonique != None, fields.values()):
        source_field = getattr(ImportSyntheseData, field.source_field)
        # This CTE return the list of source value / cd_nomenclature for a given nomenclature type
        cte = (
            select(
                [column("key").label("value"), column("value").label("cd_nomenclature")]
            )
            .select_from(
                sa.func.JSON_EACH_TEXT(TImports.contentmapping[field.mnemonique])
            )
            .where(TImports.id_import == imprt.id_import)
            .cte("cte")
        )
        # This statement join the cte results with nomenclatures
        # in order to set the id_nomenclature
        stmt = (
            update(ImportSyntheseData)
            .where(ImportSyntheseData.imprt == imprt)
            .where(source_field == cte.c.value)
            .where(TNomenclatures.cd_nomenclature == cte.c.cd_nomenclature)
            .where(BibNomenclaturesTypes.mnemonique == field.mnemonique)
            .where(TNomenclatures.id_type == BibNomenclaturesTypes.id_type)
            .values({field.synthese_field: TNomenclatures.id_nomenclature})
        )
        db.session.execute(stmt)
        # TODO: Check conditional values

    if current_app.config["IMPORT"]["FILL_MISSING_NOMENCLATURE_WITH_DEFAULT_VALUE"]:
        for field in BibFields.query.filter(BibFields.mnemonique != None).all():
            fields[field.name_field] = field
            # Set default nomenclature for empty user fields
            (
                ImportSyntheseData.query.filter(ImportSyntheseData.imprt == imprt)
                .filter(sa.or_(source_field == None, source_field == ""))
                .update(
                    values={
                        field.synthese_field: sa.func.gn_synthese.get_default_nomenclature_value(
                            field.mnemonique,
                        ),
                    },
                    synchronize_session=False,
                )
            )

    for field in filter(lambda field: field.mnemonique != None, fields.values()):
        source_field = getattr(ImportSyntheseData, field.source_field)
        synthese_field = getattr(ImportSyntheseData, field.synthese_field)
        # Note: with a correct contentmapping, no errors should occur.
        report_erroneous_rows(
            imprt,
            error_type="INVALID_NOMENCLATURE",
            error_column=field.name_field,
            whereclause=sa.and_(
                source_field != None,
                source_field != "",
                synthese_field == None,
            )
        )


def load_import_data_in_dataframe(imprt, fields):
    source_cols = [
        "id_import",
        "line_no",
        "valid",
    ] + [field.source_column for field in fields.values()]
    records = (
        db.session.query(*[ImportSyntheseData.__table__.c[col] for col in source_cols])
        .filter(
            ImportSyntheseData.imprt == imprt,
        )
        .all()
    )
    df = pd.DataFrame.from_records(
        records,
        columns=source_cols,
    )
    return df


def update_import_data_from_dataframe(imprt, fields, df):
    db.session.query(ImportSyntheseData).filter_by(id_import=imprt.id_import).update(
        {"valid": False}
    )
    if not len(df[df["valid"] == True]):
        return
    updated_cols = [
        "id_import",
        "line_no",
        "valid",
    ]
    updated_cols += [
        field.synthese_field for field in fields.values() if field.synthese_field
    ]
    df.replace({np.nan: None}, inplace=True)
    df.replace({pd.NaT: None}, inplace=True)
    records = df[df["valid"] == True][updated_cols].to_dict(orient="records")
    insert_stmt = pg_insert(ImportSyntheseData)
    insert_stmt = insert_stmt.values(records).on_conflict_do_update(
        index_elements=updated_cols[:2],
        set_={col: insert_stmt.excluded[col] for col in updated_cols[2:]},
    )
    db.session.execute(insert_stmt)


def set_the_geom_column(imprt, fields, df):
    file_srid = imprt.srid
    local_srid = db.session.execute(
        sa.func.Find_SRID("ref_geo", "l_areas", "geom")
    ).scalar()
    geom_col = df[df["_geom"].notna()]["_geom"]
    if file_srid == 4326:
        df["the_geom_4326"] = geom_col.apply(
            lambda geom: ST_GeomFromWKB(geom.wkb, file_srid)
        )
        fields["the_geom_4326"] = BibFields.query.filter_by(
            name_field="the_geom_4326"
        ).one()
    elif file_srid == local_srid:
        df["the_geom_local"] = geom_col.apply(
            lambda geom: ST_GeomFromWKB(geom.wkb, file_srid)
        )
        fields["the_geom_local"] = BibFields.query.filter_by(
            name_field="the_geom_local"
        ).one()
    else:
        df["the_geom_4326"] = geom_col.apply(
            lambda geom: ST_Transform(ST_GeomFromWKB(geom.wkb, file_srid), 4326)
        )
        fields["the_geom_4326"] = BibFields.query.filter_by(
            name_field="the_geom_4326"
        ).one()


def set_column_from_referential(imprt, field, reference, error_type):
    source_field = getattr(ImportSyntheseData, field.source_field)
    synthese_field = getattr(ImportSyntheseData, field.synthese_field)
    stmt = (
        update(ImportSyntheseData)
        .values({
            synthese_field: reference,
        })
        .where(
            source_field == sa.cast(reference, sa.Unicode)
        )
    )
    db.session.execute(stmt)
    report_erroneous_rows(
        imprt,
        error_type=error_type,
        error_column=field.name_field,
        whereclause=sa.and_(
            source_field != None,
            source_field != "",
            synthese_field == None,
        )
    )


def set_cd_nom(imprt, fields):
    if "cd_nom" not in fields:
        return
    field = fields["cd_nom"]
    set_column_from_referential(imprt, field, Taxref.cd_nom, "CD_NOM_NOT_FOUND")


def set_cd_hab(imprt, fields):
    if "cd_hab" not in fields:
        return
    field = fields["cd_hab"]
    set_column_from_referential(imprt, field, Habref.cd_hab, "CD_HAB_NOT_FOUND")


def check_mandatory_fields(imprt, fields):
    for field in fields.values():
        if not field.mandatory or not field.synthese_field:
            continue
        source_field = getattr(ImportSyntheseData, field.source_column)
        whereclause = sa.or_(
            source_field == None,
            source_field == "",
        )
        report_erroneous_rows(
            imprt,
            error_type="MISSING_VALUE",
            error_column=field.name_field,  # TODO: convert to csv col name
            whereclause=whereclause,
        )


def check_duplicates_source_pk(imprt, fields):
    if "entity_source_pk_value" not in fields:
        return
    field = fields["entity_source_pk_value"]
    synthese_field = getattr(ImportSyntheseData, field.synthese_field)
    partitions = (
        select([
            array_agg(ImportSyntheseData.line_no).over(
                partition_by=synthese_field,
            ).label("duplicate_lines")
        ])
        .where(sa.and_(
            ImportSyntheseData.imprt == imprt,
            synthese_field != None,
        ))
        .alias("partitions")
    )
    duplicates = (
        select([func.unnest(partitions.c.duplicate_lines).label("lines")])
        .where(func.array_length(partitions.c.duplicate_lines, 1) > 1)
        .alias("duplicates")
    )
    report_erroneous_rows(
        imprt,
        error_type="DUPLICATE_ENTITY_SOURCE_PK",
        error_column=field.name_field,  # TODO: convert to csv col name
        whereclause=(
            ImportSyntheseData.line_no == duplicates.c.lines
        ),
    )


def set_geom_from_area_code(imprt, source_column, area_type_filter):
    # Find area in CTE, then update corresponding column in statement
    cte = (
        ImportSyntheseData.query
        .filter(
            ImportSyntheseData.imprt == imprt,
            ImportSyntheseData.valid == True,
            ImportSyntheseData.the_geom_4326 == None,
        )
        .join(
            LAreas, LAreas.area_code == source_column,
        )
        .join(
            LAreas.area_type, aliased=True,
        )
        .filter(area_type_filter)
        .with_entities(
            ImportSyntheseData.id_import,
            ImportSyntheseData.line_no,
            LAreas.id_area,
            LAreas.geom,
        )
        .cte("cte")
    )
    stmt = (
        update(ImportSyntheseData)
        .values({
            ImportSyntheseData.id_area_attachment: cte.c.id_area,
            ImportSyntheseData.the_geom_local: cte.c.geom,
            ImportSyntheseData.the_geom_4326: ST_Transform(cte.c.geom, 4326),
        })
        .where(ImportSyntheseData.id_import == cte.c.id_import)
        .where(ImportSyntheseData.line_no == cte.c.line_no)
    )
    db.session.execute(stmt)


def report_erroneous_rows(imprt, error_type, error_column, whereclause):
    cte = (
        update(ImportSyntheseData)
        .values({
            ImportSyntheseData.valid: False,
        })
        .where(ImportSyntheseData.imprt == imprt)
        .where(whereclause)
        .returning(ImportSyntheseData.line_no)
        .cte("cte")
    )
    error = (
        select([
            literal(imprt.id_import).label("id_import"),
            ImportUserErrorType.query.filter_by(
                name=error_type,
            )
            .with_entities(ImportUserErrorType.pk)
            .label("id_type"),
            array_agg(
                aggregate_order_by(cte.c.line_no, cte.c.line_no),
            )
            .label("rows"),
            literal(error_column),
        ])
        .alias("error")
    )
    stmt = (
        insert(ImportUserError)
        .from_select(
            names=[
                ImportUserError.id_import,
                ImportUserError.id_type,
                ImportUserError.rows,
                ImportUserError.column,
            ],
            select=(
                select([error])
                .where(error.c.rows != None)
            ),
        )
    )
    db.session.execute(stmt)


def complete_others_geom_columns(imprt, fields):
    local_srid = db.session.execute(
        sa.func.Find_SRID("ref_geo", "l_areas", "geom")
    ).scalar()
    if "the_geom_4326" not in fields:
        assert "the_geom_local" in fields
        source_col = "the_geom_local"
        dest_col = "the_geom_4326"
        dest_srid = 4326
    else:
        assert "the_geom_4326" in fields
        source_col = "the_geom_4326"
        dest_col = "the_geom_local"
        dest_srid = local_srid
    (
        ImportSyntheseData.query
        .filter(
            ImportSyntheseData.imprt == imprt,
            ImportSyntheseData.valid == True,
            getattr(ImportSyntheseData, source_col) != None,
        )
        .update(
            values={
                dest_col: ST_Transform(
                    getattr(ImportSyntheseData, source_col), dest_srid
                ),
            },
            synchronize_session=False,
        )
    )
    for name_field, area_type_filter in [
        ("codecommune", BibAreasTypes.type_code == "COM"),
        ("codedepartement", BibAreasTypes.type_code == "DEP"),
        ("codemaille", BibAreasTypes.type_code.in_(["M1", "M5", "M10"]))
    ]:
        if name_field not in fields:
            continue
        field = fields[name_field]
        source_column = getattr(ImportSyntheseData, field.source_field)
        # Set geom from area of the given type and with matching area_code:
        set_geom_from_area_code(imprt, source_column, area_type_filter)
        # Mark rows with code specified but geom still empty as invalid:
        report_erroneous_rows(
            imprt,
            error_type="INVALID_ATTACHMENT_CODE",
            error_column=field.name_field,  # TODO: convert to csv col name
            whereclause=sa.and_(
                ImportSyntheseData.the_geom_4326 == None,
                ImportSyntheseData.valid == True,
                source_column != None,
                source_column != "",
            ),
        )
    # Mark rows with no geometry as invalid:
    report_erroneous_rows(
        imprt,
        error_type="NO-GEOM",
        error_column="Colonnes géométriques",
        whereclause=sa.and_(
            ImportSyntheseData.the_geom_4326 == None,
            ImportSyntheseData.valid == True,
        ),
    )
    # Set the_geom_point:
    (
        ImportSyntheseData.query
        .filter(
            ImportSyntheseData.imprt == imprt,
            ImportSyntheseData.valid == True,
        )
        .update(
            values={
                ImportSyntheseData.the_geom_point: ST_Centroid(ImportSyntheseData.the_geom_4326),
            },
            synchronize_session=False,
        )
    )


def toggle_synthese_triggers(enable):
    triggers = ["tri_meta_dates_change_synthese", "tri_insert_cor_area_synthese"]
    action = "ENABLE" if enable else "DISABLE"
    with db.session.begin_nested():
        for trigger in triggers:
            db.session.execute(
                f"ALTER TABLE gn_synthese.synthese {action} TRIGGER {trigger}"
            )


def import_data_to_synthese(imprt, source):
    generated_fields = {
        "datetime_min",
        "datetime_max",
        "the_geom_4326",
        "the_geom_local",
        "the_geom_point",
        "id_area_attachment",
    }
    # TODO: altitude? uuid? (when not in mapping)
    fields = BibFields.query.filter(
        BibFields.synthese_field != None,
        BibFields.name_field.in_(imprt.fieldmapping.keys() | generated_fields),
    ).all()
    select_stmt = (
        ImportSyntheseData.query.filter_by(imprt=imprt, valid=True)
        .with_entities(
            *[getattr(ImportSyntheseData, field.synthese_field) for field in fields]
        )
        .add_columns(
            literal(source.id_source),
            literal(TModules.query.filter_by(module_code=MODULE_CODE).one().id_module),
            literal(imprt.id_dataset),
            literal("I"),
        )
    )
    names = [field.synthese_field for field in fields] + [
        "id_source",
        "id_module",
        "id_dataset",
        "last_action",
    ]
    insert_stmt = insert(Synthese).from_select(
        names=names,
        select=select_stmt,
    )
    db.session.execute(insert_stmt)


def populate_cor_area_synthese(imprt, source):
    # Populate synthese / area association table
    # A synthese entry is associated to an area when the area is enabled,
    # and when the synthese geom intersects with the area
    # (we also check the intersection is more than just touches when the geom is not a point)
    synthese_geom = Synthese.__table__.c.the_geom_local
    area_geom = LAreas.__table__.c.geom
    stmt = corAreaSynthese.insert().from_select(
        names=[
            corAreaSynthese.c.id_synthese,
            corAreaSynthese.c.id_area,
        ],
        select=select(
            [
                Synthese.__table__.c.id_synthese,
                LAreas.__table__.c.id_area,
            ]
        )
        .select_from(
            Synthese.__table__.join(
                LAreas.__table__,
                sa.func.ST_Intersects(synthese_geom, area_geom),
            )
        )
        .where(
            (LAreas.__table__.c.enable == True)
            & (
                (sa.func.ST_GeometryType(synthese_geom) == "ST_Point")
                | ~(sa.func.ST_Touches(synthese_geom, area_geom))
            )
            & (Synthese.__table__.c.id_source == source.id_source)
        ),
    )
    db.session.execute(stmt)
