from io import StringIO
import csv
import json

from flask import stream_with_context
from sqlalchemy import func
from chardet.universaldetector import UniversalDetector

from geonature.utils.env import db

from gn_module_import.models import BibFields, ImportSyntheseData


def get_valid_bbox(imprt):
    stmt = (
        db.session.query(
            func.ST_AsGeojson(
                func.ST_Extent(
                    ImportSyntheseData.the_geom_4326
                )
            )
        )
        .filter(
            ImportSyntheseData.imprt == imprt,
            ImportSyntheseData.valid == True,
        )
    )
    valid_bbox, = db.session.execute(stmt).fetchone()
    if valid_bbox:
        return json.loads(valid_bbox)


def insert_import_data_in_database(imprt):
    columns = imprt.columns
    extra_columns = set(columns) - set(imprt.fieldmapping.values())
    csvfile = StringIO(imprt.source_file.decode(imprt.encoding))
    csvreader = csv.DictReader(csvfile, fieldnames=columns, delimiter=';')  # imprt.delimiter)
    header = next(csvreader, None)  # skip header
    for key, value in header.items():  # FIXME
        assert key == value
    fields = BibFields.query.filter_by(autogenerated=False).all()
    fieldmapping = {
        field.source_column: imprt.fieldmapping[field.name_field]
        for field in fields
        if (
            field.name_field in imprt.fieldmapping
            and imprt.fieldmapping[field.name_field] in columns
        )
    }
    obs = []
    line_no = 0
    for row in csvreader:
        line_no += 1
        assert list(row.keys()) == columns
        o = {
            'id_import': imprt.id_import,
            'line_no': line_no,
        }
        o.update({
            dest_field: row[source_field]
            for dest_field, source_field in fieldmapping.items()
        })
        o.update({
            'extra_fields': {
                col: row[col]
                for col in extra_columns
            },
        })
        obs.append(o)
        if len(obs) > 1000:
            db.session.bulk_insert_mappings(ImportSyntheseData, obs)
            obs = []
    if obs:
        db.session.bulk_insert_mappings(ImportSyntheseData, obs)
    return line_no


def detect_encoding(f):
    detector = UniversalDetector()
    for row in f:
        detector.feed(row)
        if detector.done:
            break
    f.seek(0)
    detector.close()
    return detector.result['encoding']


@stream_with_context
def stream_csv(rows):
    data = StringIO()
    w = csv.writer(data)
    for row in rows:
        w.writerow(row)
        yield data.getvalue()
        data.seek(0)
        data.truncate(0)
