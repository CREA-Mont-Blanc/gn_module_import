from io import StringIO
import csv
import logging
import json
import numpy as np
from shapely import wkb

from slugify import slugify
from flask import current_app, stream_with_context
from werkzeug.exceptions import BadRequest
from sqlalchemy.schema import Table
from sqlalchemy import func
import pandas as pd
from chardet.universaldetector import UniversalDetector
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy import cast as sa_cast, Numeric, DateTime

from geonature.utils.env import DB as db

from gn_module_import.models import BibFields, ImportSyntheseData


MAX_TABLE_NAME_LEN = 30
MAX_COLUMN_NAME_LEN = 40


def get_valid_bbox(imprt):
    stmt = (
        db.session.query(
            func.ST_AsGeojson(
                func.ST_Extent(
                    ImportSyntheseData.the_geom_4326
                )
            )
        )
        .filter(
            ImportSyntheseData.imprt==imprt,
            ImportSyntheseData.valid==True,
        )
    )
    valid_bbox, = db.session.execute(stmt).fetchone()
    if valid_bbox:
        return json.loads(valid_bbox)


#def load_geojson_data(imprt, geojson_data):
#    csv_data = StringIO()
#    #  parse_geojson(geojson_data, csv_data, geometry_col_name)
#    raise Exception("Not implemented")
#    return load_csv_data(imprt, csv_data.encode('utf-8'), encoding='utf-8')


#def load_csv_data(imprt, csv_data, encoding):
#    csvfile = TextIOWrapper(csv_data, encoding=encoding)
#    headline = csvfile.readline()
#    dialect = csv.Sniffer().sniff(headline)
#    csvreader = csv.reader(csvfile, delimiter=dialect.delimiter)
#    csvfile.seek(0)
#    columns = next(csvreader)
#    duplicates = set([col for col in columns if columns.count(col) > 1])
#    if duplicates:
#        raise BadRequest(f"Duplicates column names: {duplicates}")
#    imprt.columns = {col: get_clean_column_name(col) for col in columns}
#    csvfile.seek(0)
#    try:
#        df = pd.read_csv(csvfile, delimiter=dialect.delimiter,
#                         header=0,  # this will determine the "official" cols number
#                         index_col=None,
#                         on_bad_lines='error')
#    except pd.errors.ParserError as e:
#        raise BadRequest(description=str(e))
#    # Use slugified db-compatible column names
#    df = df.reindex(columns=imprt.columns.values())
#    return df


#def load_data(imprt, raw_data, encoding, fmt):
#    if fmt == 'csv':
#        df = load_csv_data(imprt, BytesIO(raw_data), encoding=encoding)
#    elif fmt == 'geojson':
#        try:
#            data = raw_data.decode(encoding)
#        except UnicodeDecodeError:
#            raise BadRequest(description='Erreur dâ€™encodage')
#        df = load_geojson_data(imprt, data)
#    return df


def insert_import_data_in_database(imprt):
    columns = imprt.columns
    extra_columns = set(columns) - set(imprt.fieldmapping.values())
    csvfile = StringIO(imprt.source_file.decode(imprt.encoding))
    csvreader = csv.DictReader(csvfile, fieldnames=columns, delimiter=';')#imprt.delimiter)
    header = next(csvreader, None)  # skip header
    for key, value in header.items():  # FIXME
        assert key == value
    fields = BibFields.query.filter_by(autogenerated=False).all()
    fieldmapping = {
        field.source_column: imprt.fieldmapping[field.name_field]
        for field in fields
        if (
            field.name_field in imprt.fieldmapping
            and imprt.fieldmapping[field.name_field] in columns
        )
    }
    obs = []
    line_no = 0
    for row in csvreader:
        line_no += 1
        assert list(row.keys()) == columns
        o = {
            'id_import': imprt.id_import,
            'line_no': line_no,
        }
        o.update({
            dest_field: row[source_field]
            for dest_field, source_field in fieldmapping.items()
        })
        o.update({
            'extra_fields': {
                col: row[col]
                for col in extra_columns
            },
        })
        obs.append(o)
        if len(obs) > 1000:
            #with db.session.begin_nested():
            db.session.bulk_insert_mappings(ImportSyntheseData, obs)
            obs = []
    if obs:
        db.session.bulk_insert_mappings(ImportSyntheseData, obs)
    return line_no


def detect_encoding(f):
    #logger = logging.getLogger('chardet')
    #logger.setLevel(logging.CRITICAL)
    detector = UniversalDetector()
    for row in f:
        detector.feed(row)
        if detector.done:
            break
    f.seek(0)
    detector.close()
    return detector.result['encoding']


overrided_source_fields = {
    'date_min': 'concatened_date_min',
    'date_max': 'concatened_date_max',
}


def get_selected_synthese_fields(imprt):
    """
        Return a list [ fieldmapping ]
    """
    synthese_fields = {
        f.name_field: f
        for f in BibFields.query.filter_by(synthese_field=True)
    }
    return {
        synthese_fields[target]: source
        for target, source in imprt.fieldmapping.items()
        if source in imprt.columns.keys() and target in synthese_fields
    }


@stream_with_context
def stream_csv(rows):
    data = StringIO()
    w = csv.writer(data)
    for row in rows:
        w.writerow(row)
        yield data.getvalue()
        data.seek(0)
        data.truncate(0)
